{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30dff5f6-3b37-4674-97e9-68e21a8fdb8f",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# SDXL Turbo Storyboard to Video\n",
    "## Version 1.0\n",
    "\n",
    "***\n",
    "\n",
    "#### https://github.com/asigalov61/tegridy-vibe-code\n",
    "\n",
    "***\n",
    "\n",
    "### Project Los Angeles\n",
    "### Tegridy Code 2026\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a96f92a-5a27-4e7f-9c99-c51b9a3823e3",
   "metadata": {},
   "source": [
    "# Setup Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da7e53d-ca5c-412b-995f-2105be9f3c3a",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "!pip install U diffusers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a101519f-e3c6-4aa0-af7a-97d9048ee37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34ceee7-6295-4943-8dd4-20be8b47bca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ef07c5-58c0-4076-bd72-2dd7cfc39bba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -U moviepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7cff52-7dd3-4810-a7fa-8b283910b6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy==1.26.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab2f4e9-8614-4166-bc2b-8fed985941db",
   "metadata": {},
   "source": [
    "# Run the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f14d2c-a92b-4aae-8f17-6d37274fec54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Complete patched SDXL image2image morphing script\n",
    "- Style-preserving pixel-space interpolation of decoded VAE images\n",
    "- Per-frame strength interpolation to retain source style near endpoints\n",
    "- Temporal smoothing in latent space\n",
    "- Robust VAE encode/decode in float32 with NaN guards\n",
    "- Upcast VAE to float32 and disable AMP autocast during pipe(...) calls to avoid fp16 instability\n",
    "- Retry wrapper for pipeline calls with aggressive normalization and prompt-text fallback\n",
    "- Deterministic seeding per-segment and per-frame\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import csv\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from diffusers import AutoPipelineForImage2Image\n",
    "from torch.amp import autocast\n",
    "\n",
    "# -------------------------\n",
    "# User configuration\n",
    "# -------------------------\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "pipeline_torch_dtype = torch.float32\n",
    "vae_encode_dtype = torch.float32\n",
    "autocast_dtype = torch.float32\n",
    "autocast_enabled = False\n",
    "\n",
    "model_id = \"stabilityai/sdxl-turbo\"\n",
    "variant = \"fp16\"\n",
    "\n",
    "prompts: List[str] = [\n",
    "    \"On a dark desert highway,\",\n",
    "    \"cool wind in my hair\",\n",
    "    \"Warm smell of colitas rising up through the air\",\n",
    "    \"Up ahead in the distance, I saw a shimmering light\",\n",
    "]\n",
    "\n",
    "# One image per prompt or None to use noise for that endpoint\n",
    "prompt_images: Optional[List[Optional[str]]] = ['image_0.jpg', 'image_1.jpg', 'image_2.jpg', 'image_3.jpg']\n",
    "seeds = -1 # [42, 1337, 7, 14]\n",
    "\n",
    "num_steps_between = 120\n",
    "output_dir = \"frames_sdxl_img2img\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Strength controls: lower values preserve source style more\n",
    "base_strength = 0.45\n",
    "min_strength = 0.12\n",
    "\n",
    "guidance_scale = 0.0  # SDXL Turbo requirement\n",
    "num_inference_steps = 12\n",
    "\n",
    "framerate = 16\n",
    "output_video = \"sdxl_img2img_morph.mp4\"\n",
    "cleanup_frames_after_video = False\n",
    "\n",
    "DEBUG_FRAMES = 3\n",
    "DEBUG_CSV = os.path.join(output_dir, \"latent_stats.csv\")\n",
    "IMG_DEBUG_CSV = os.path.join(output_dir, \"image_encode_debug.csv\")\n",
    "\n",
    "noise_mix_scale = 0.5\n",
    "temporal_smooth_alpha = 0.24\n",
    "\n",
    "# -------------------------\n",
    "# Utility functions\n",
    "# -------------------------\n",
    "def lerp(a: torch.Tensor, b: torch.Tensor, t: float) -> torch.Tensor:\n",
    "    return a * (1.0 - t) + b * t\n",
    "\n",
    "def cosine_ease(t: float) -> float:\n",
    "    return 0.5 - 0.5 * math.cos(math.pi * t)\n",
    "\n",
    "def slerp(a: torch.Tensor, b: torch.Tensor, t: float, eps: float = 1e-6) -> torch.Tensor:\n",
    "    orig_shape = a.shape\n",
    "    a_flat = a.reshape(a.shape[0], -1)\n",
    "    b_flat = b.reshape(b.shape[0], -1)\n",
    "\n",
    "    a_norm = torch.linalg.norm(a_flat, dim=1, keepdim=True).clamp(min=eps)\n",
    "    b_norm = torch.linalg.norm(b_flat, dim=1, keepdim=True).clamp(min=eps)\n",
    "\n",
    "    a_unit = a_flat / a_norm\n",
    "    b_unit = b_flat / b_norm\n",
    "\n",
    "    dot = (a_unit * b_unit).sum(dim=1, keepdim=True).clamp(-1.0 + 1e-7, 1.0 - 1e-7)\n",
    "    omega = torch.acos(dot)\n",
    "    sin_omega = torch.sin(omega)\n",
    "\n",
    "    near_parallel = (sin_omega.abs() < 1e-3).squeeze(1)\n",
    "\n",
    "    out_flat = torch.empty_like(a_flat)\n",
    "    for i in range(a_flat.shape[0]):\n",
    "        if near_parallel[i]:\n",
    "            out_flat[i] = lerp(a_flat[i], b_flat[i], t)\n",
    "        else:\n",
    "            o = omega[i, 0]\n",
    "            s = sin_omega[i, 0]\n",
    "            factor_a = torch.sin((1.0 - t) * o) / s\n",
    "            factor_b = torch.sin(t * o) / s\n",
    "            out_flat[i] = factor_a * a_flat[i] + factor_b * b_flat[i]\n",
    "\n",
    "    mag = lerp(a_norm, b_norm, t)\n",
    "    out_flat = out_flat / torch.linalg.norm(out_flat, dim=1, keepdim=True).clamp(min=eps) * mag\n",
    "\n",
    "    return out_flat.reshape(orig_shape)\n",
    "\n",
    "def ensure_list_of_seeds(seeds_input, n: int) -> List[int]:\n",
    "    if isinstance(seeds_input, int):\n",
    "        return [seeds_input] * n\n",
    "    if isinstance(seeds_input, (list, tuple)):\n",
    "        if len(seeds_input) == n:\n",
    "            return list(seeds_input)\n",
    "        if len(seeds_input) == 1:\n",
    "            return [seeds_input[0]] * n\n",
    "        raise ValueError(f\"seeds must be an int or a list of length 1 or {n}\")\n",
    "    raise ValueError(\"seeds must be an int or a list/tuple of ints\")\n",
    "\n",
    "def ensure_list_of_images(images_input, n: int) -> List[Optional[str]]:\n",
    "    if images_input is None:\n",
    "        return [None] * n\n",
    "    if isinstance(images_input, str):\n",
    "        return [images_input] * n\n",
    "    if isinstance(images_input, (list, tuple)):\n",
    "        if len(images_input) == n:\n",
    "            return list(images_input)\n",
    "        if len(images_input) == 1:\n",
    "            return [images_input[0]] * n\n",
    "        raise ValueError(f\"prompt_images must be None, a str, or a list of length 1 or {n}\")\n",
    "    raise ValueError(\"prompt_images must be None, a str, or a list/tuple of paths or None\")\n",
    "\n",
    "# -------------------------\n",
    "# Load pipeline\n",
    "# -------------------------\n",
    "print(\"Loading image2image pipeline...\")\n",
    "load_kwargs = {\"torch_dtype\": pipeline_torch_dtype}\n",
    "if variant:\n",
    "    load_kwargs[\"variant\"] = variant\n",
    "\n",
    "pipe = AutoPipelineForImage2Image.from_pretrained(model_id, **load_kwargs).to(device)\n",
    "pipe.set_progress_bar_config(disable=True)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "if not hasattr(pipe, \"encode_prompt\"):\n",
    "    raise RuntimeError(\"encode_prompt not available. Upgrade diffusers.\")\n",
    "\n",
    "init_sigma_raw = getattr(pipe.scheduler, \"init_noise_sigma\", None)\n",
    "scheduler_init_sigma = float(init_sigma_raw) if init_sigma_raw is not None else 1.0\n",
    "print(\"Scheduler init sigma:\", scheduler_init_sigma)\n",
    "\n",
    "# -------------------------\n",
    "# Prompt encoding\n",
    "# -------------------------\n",
    "def encode_single(prompt: str) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    out = pipe.encode_prompt(\n",
    "        prompt=prompt,\n",
    "        device=device,\n",
    "        num_images_per_prompt=1,\n",
    "        do_classifier_free_guidance=False,\n",
    "        negative_prompt=None,\n",
    "    )\n",
    "    if isinstance(out, tuple):\n",
    "        prompt_embeds = out[0]\n",
    "        pooled_prompt_embeds = out[1] if len(out) > 1 else None\n",
    "    else:\n",
    "        prompt_embeds = out\n",
    "        pooled_prompt_embeds = None\n",
    "    prompt_embeds = prompt_embeds.to(device=device, dtype=pipeline_torch_dtype)\n",
    "    if pooled_prompt_embeds is None:\n",
    "        proj_dim = getattr(pipe.text_encoder_2.config, \"projection_dim\", 1280)\n",
    "        pooled_prompt_embeds = torch.zeros((1, proj_dim), device=device, dtype=pipeline_torch_dtype)\n",
    "    else:\n",
    "        pooled_prompt_embeds = pooled_prompt_embeds.to(device=device, dtype=pipeline_torch_dtype)\n",
    "    return prompt_embeds, pooled_prompt_embeds\n",
    "\n",
    "def encode_prompts(prompts_list: List[str]):\n",
    "    encoded = []\n",
    "    max_seq_len = 0\n",
    "    for p in prompts_list:\n",
    "        pe, pooled = encode_single(p)\n",
    "        seq_len = pe.shape[1]\n",
    "        if seq_len > max_seq_len:\n",
    "            max_seq_len = seq_len\n",
    "        encoded.append((pe, pooled))\n",
    "    return encoded, max_seq_len\n",
    "\n",
    "def pad_prompt_embeds(pe: torch.Tensor, target_seq_len: int) -> torch.Tensor:\n",
    "    seq_len = pe.shape[1]\n",
    "    if seq_len == target_seq_len:\n",
    "        return pe\n",
    "    dim = pe.shape[2]\n",
    "    padded = torch.zeros((1, target_seq_len, dim), device=device, dtype=pipeline_torch_dtype)\n",
    "    padded[:, :seq_len, :] = pe\n",
    "    return padded\n",
    "\n",
    "# -------------------------\n",
    "# Image to latent and latent to PIL helpers\n",
    "# -------------------------\n",
    "def load_image(path_or_pil):\n",
    "    if isinstance(path_or_pil, Image.Image):\n",
    "        return path_or_pil.convert(\"RGB\")\n",
    "    return Image.open(path_or_pil).convert(\"RGB\")\n",
    "\n",
    "def preprocess_image_for_vae_fp32(img: Image.Image, target_pixel: int) -> torch.Tensor:\n",
    "    transform = T.Compose([\n",
    "        T.Resize((target_pixel, target_pixel), interpolation=T.InterpolationMode.LANCZOS),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
    "    ])\n",
    "    tensor = transform(img).unsqueeze(0).to(device=device, dtype=vae_encode_dtype)\n",
    "    return tensor\n",
    "\n",
    "def has_nan(t: torch.Tensor) -> bool:\n",
    "    return torch.isnan(t).any().item()\n",
    "\n",
    "def encode_image_to_latent(path_or_pil) -> torch.Tensor:\n",
    "    img = load_image(path_or_pil)\n",
    "    sample_size = getattr(pipe.unet.config, \"sample_size\", None)\n",
    "    if sample_size is None:\n",
    "        raise RuntimeError(\"Cannot determine UNet sample_size from pipeline\")\n",
    "    target_pixel = int(sample_size * 8)\n",
    "    img_tensor = preprocess_image_for_vae_fp32(img, target_pixel)\n",
    "\n",
    "    # Log preprocess stats\n",
    "    img_min = float(img_tensor.min().cpu())\n",
    "    img_max = float(img_tensor.max().cpu())\n",
    "    img_mean = float(img_tensor.mean().cpu())\n",
    "    img_std = float(img_tensor.std().cpu())\n",
    "    if not os.path.exists(IMG_DEBUG_CSV):\n",
    "        with open(IMG_DEBUG_CSV, \"w\", newline=\"\") as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow([\"image_path\", \"tensor_dtype\", \"tensor_shape\", \"min\", \"max\", \"mean\", \"std\"])\n",
    "    with open(IMG_DEBUG_CSV, \"a\", newline=\"\") as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow([getattr(path_or_pil, \"filename\", str(path_or_pil)), str(img_tensor.dtype), tuple(img_tensor.shape), f\"{img_min:.8f}\", f\"{img_max:.8f}\", f\"{img_mean:.8f}\", f\"{img_std:.8f}\"])\n",
    "\n",
    "    if has_nan(img_tensor):\n",
    "        img_tensor = torch.nan_to_num(img_tensor, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "    if not (hasattr(pipe, \"vae\") and pipe.vae is not None):\n",
    "        raise RuntimeError(\"No VAE found on pipeline\")\n",
    "\n",
    "    vae = pipe.vae\n",
    "    vae_device = next(vae.parameters()).device\n",
    "    orig_vae_dtype = next(vae.parameters()).dtype\n",
    "\n",
    "    need_cast_back = False\n",
    "    if orig_vae_dtype == torch.float16:\n",
    "        vae.to(torch.float32)\n",
    "        need_cast_back = True\n",
    "\n",
    "    img_for_encode = img_tensor.to(device=vae_device, dtype=torch.float32)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if device.startswith(\"cuda\"):\n",
    "            # from torch.amp import autocast\n",
    "            with autocast(\"cuda\", dtype=autocast_dtype, enabled=autocast_enabled):\n",
    "                enc = vae.encode(img_for_encode)\n",
    "        else:\n",
    "            enc = vae.encode(img_for_encode)\n",
    "\n",
    "    if need_cast_back:\n",
    "        vae.to(orig_vae_dtype)\n",
    "\n",
    "    lat = None\n",
    "    if enc is None:\n",
    "        raise RuntimeError(\"pipe.vae.encode returned None\")\n",
    "\n",
    "    try:\n",
    "        if hasattr(enc, \"latent_dist\") and hasattr(enc.latent_dist, \"mean\"):\n",
    "            cand = enc.latent_dist.mean.to(device=vae_device, dtype=torch.float32)\n",
    "            if not torch.isnan(cand).any():\n",
    "                lat = cand\n",
    "            else:\n",
    "                try:\n",
    "                    samp = enc.latent_dist.sample().to(device=vae_device, dtype=torch.float32)\n",
    "                    if not torch.isnan(samp).any():\n",
    "                        lat = samp\n",
    "                except Exception:\n",
    "                    pass\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    if lat is None:\n",
    "        try:\n",
    "            if isinstance(enc, (list, tuple)) and len(enc) > 0 and isinstance(enc[0], torch.Tensor):\n",
    "                cand = enc[0].to(device=vae_device, dtype=torch.float32)\n",
    "                if not torch.isnan(cand).any():\n",
    "                    lat = cand\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    if lat is None and isinstance(enc, torch.Tensor):\n",
    "        cand = enc.to(device=vae_device, dtype=torch.float32)\n",
    "        if not torch.isnan(cand).any():\n",
    "            lat = cand\n",
    "\n",
    "    if lat is None:\n",
    "        try:\n",
    "            cand = enc.latent_dist.mean.to(device=vae_device, dtype=torch.float32)\n",
    "            if torch.isnan(cand).any():\n",
    "                cand = torch.nan_to_num(cand, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "            lat = cand\n",
    "            print(f\"[IMG DEBUG] used fallback latent (NaNs replaced) for {getattr(path_or_pil, 'filename', str(path_or_pil))}\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to extract any latent candidate: {e}\")\n",
    "\n",
    "    if has_nan(lat):\n",
    "        lat = torch.nan_to_num(lat, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "    lat = lat.to(device=device, dtype=torch.float32)\n",
    "\n",
    "    vae_scaling = getattr(pipe.vae.config, \"scaling_factor\", 0.13025)\n",
    "    if vae_scaling is None:\n",
    "        vae_scaling = 0.13025\n",
    "    lat = lat * float(vae_scaling)\n",
    "\n",
    "    # print(f\"[IMG DEBUG] Final image latent for {getattr(path_or_pil, 'filename', str(path_or_pil))}: dtype {lat.dtype} shape {lat.shape} min {float(lat.min()):.6f} max {float(lat.max()):.6f} std {float(lat.std()):.6f}\")\n",
    "\n",
    "    return lat\n",
    "\n",
    "def latent_to_pil(latent: torch.Tensor) -> Image.Image:\n",
    "    if latent is None:\n",
    "        raise ValueError(\"latent is None\")\n",
    "\n",
    "    vae_scaling = getattr(pipe.vae.config, \"scaling_factor\", 0.13025)\n",
    "    lat_for_decode = (latent / float(vae_scaling)).to(dtype=torch.float32)\n",
    "\n",
    "    vae = pipe.vae\n",
    "    vae_device = next(vae.parameters()).device\n",
    "    orig_vae_dtype = next(vae.parameters()).dtype\n",
    "\n",
    "    need_cast_back = False\n",
    "    if orig_vae_dtype == torch.float16:\n",
    "        vae.to(torch.float32)\n",
    "        need_cast_back = True\n",
    "\n",
    "    noisy_for_decode = lat_for_decode.to(device=vae_device, dtype=torch.float32)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if device.startswith(\"cuda\"):\n",
    "            # from torch.amp import autocast\n",
    "            with autocast(\"cuda\", dtype=autocast_dtype, enabled=autocast_enabled):\n",
    "                decoded = vae.decode(noisy_for_decode)\n",
    "        else:\n",
    "            decoded = vae.decode(noisy_for_decode)\n",
    "\n",
    "    if need_cast_back:\n",
    "        vae.to(orig_vae_dtype)\n",
    "\n",
    "    decoded_tensor = None\n",
    "    if isinstance(decoded, torch.Tensor):\n",
    "        decoded_tensor = decoded\n",
    "    else:\n",
    "        sample_attr = getattr(decoded, \"sample\", None)\n",
    "        if callable(sample_attr):\n",
    "            try:\n",
    "                decoded_tensor = sample_attr()\n",
    "            except Exception:\n",
    "                decoded_tensor = None\n",
    "        if decoded_tensor is None and sample_attr is not None and not callable(sample_attr):\n",
    "            try:\n",
    "                if isinstance(sample_attr, torch.Tensor):\n",
    "                    decoded_tensor = sample_attr\n",
    "                else:\n",
    "                    decoded_tensor = torch.as_tensor(sample_attr)\n",
    "            except Exception:\n",
    "                decoded_tensor = None\n",
    "        if decoded_tensor is None and isinstance(decoded, (list, tuple)) and len(decoded) > 0:\n",
    "            cand = decoded[0]\n",
    "            try:\n",
    "                decoded_tensor = torch.as_tensor(cand) if not isinstance(cand, torch.Tensor) else cand\n",
    "            except Exception:\n",
    "                decoded_tensor = None\n",
    "        if decoded_tensor is None and isinstance(decoded, dict):\n",
    "            for key in (\"sample\", \"reconstruction\", \"recon\", \"decoded\"):\n",
    "                if key in decoded:\n",
    "                    cand = decoded[key]\n",
    "                    try:\n",
    "                        decoded_tensor = torch.as_tensor(cand) if not isinstance(cand, torch.Tensor) else cand\n",
    "                        break\n",
    "                    except Exception:\n",
    "                        decoded_tensor = None\n",
    "\n",
    "    if decoded_tensor is None:\n",
    "        raise RuntimeError(\"Unexpected VAE.decode return type; cannot convert to a tensor for image conversion.\")\n",
    "\n",
    "    decoded_tensor = decoded_tensor.to(device=\"cpu\", dtype=torch.float32)\n",
    "    decoded_tensor = (decoded_tensor.clamp(-1.0, 1.0) + 1.0) / 2.0\n",
    "    decoded_arr = (decoded_tensor * 255.0).permute(0, 2, 3, 1).cpu().numpy().astype(\"uint8\")\n",
    "    pil = Image.fromarray(decoded_arr[0])\n",
    "    return pil\n",
    "\n",
    "# -------------------------\n",
    "# Robust pipeline call helper with VAE upcast and autocast disabled\n",
    "# -------------------------\n",
    "def safe_call_pipe_with_image(pipe, *, prompt_embeds=None, pooled_prompt_embeds=None,\n",
    "                              prompt_text: Optional[str]=None, image: Image.Image,\n",
    "                              strength: float, num_inference_steps: int,\n",
    "                              guidance_scale: float, generator: torch.Generator,\n",
    "                              output_type: str = \"pil\", max_retries: int = 2,\n",
    "                              min_scheduler_steps: int = 12, retry_with_steps: int = 20):\n",
    "    \"\"\"\n",
    "    Robust wrapper for image2image pipeline with additional scheduler guards and\n",
    "    automatic retry with more steps when very small num_inference_steps cause failures.\n",
    "\n",
    "    - Ensures image is RGB and target size.\n",
    "    - Ensures scheduler has at least `min_scheduler_steps` timesteps.\n",
    "    - Temporarily upcasts VAE to float32 and disables autocast for the call.\n",
    "    - Retries normalization attempts; if still failing and original num_inference_steps\n",
    "      is small, retries once with `retry_with_steps` steps.\n",
    "    - Falls back to prompt-text call if prompt_embeds path fails.\n",
    "    \"\"\"\n",
    "    # Normalize image\n",
    "    if image.mode != \"RGB\":\n",
    "        image = image.convert(\"RGB\")\n",
    "\n",
    "    target_px = pipe.unet.config.sample_size * 8\n",
    "    if image.size != (target_px, target_px):\n",
    "        image = image.resize((target_px, target_px), resample=Image.LANCZOS)\n",
    "\n",
    "    # Ensure scheduler timesteps are sane for this call\n",
    "    try:\n",
    "        # Use at least min_scheduler_steps to avoid degenerate scheduler arrays\n",
    "        safe_steps = max(int(num_inference_steps), int(min_scheduler_steps))\n",
    "        pipe.scheduler.set_timesteps(safe_steps)\n",
    "        timesteps = getattr(pipe.scheduler, \"timesteps\", None)\n",
    "        # If timesteps is None or empty, force a small valid set\n",
    "        if timesteps is None or (hasattr(timesteps, \"__len__\") and len(timesteps) == 0):\n",
    "            pipe.scheduler.set_timesteps(max(safe_steps, min_scheduler_steps))\n",
    "            timesteps = pipe.scheduler.timesteps\n",
    "    except Exception:\n",
    "        # If scheduler manipulation fails, still proceed but mark timesteps invalid\n",
    "        timesteps = getattr(pipe.scheduler, \"timesteps\", None)\n",
    "\n",
    "    vae = pipe.vae\n",
    "    orig_vae_dtype = next(vae.parameters()).dtype\n",
    "\n",
    "    last_exc = None\n",
    "    attempted_with_increased_steps = False\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            # Upcast VAE to float32 for stable encode/decode if needed\n",
    "            need_cast_back = False\n",
    "            if orig_vae_dtype == torch.float16:\n",
    "                vae.to(torch.float32)\n",
    "                need_cast_back = True\n",
    "\n",
    "            # Ensure prompt_embeds are on correct device/dtype\n",
    "            if prompt_embeds is not None:\n",
    "                prompt_embeds = prompt_embeds.to(device=device, dtype=pipeline_torch_dtype)\n",
    "            if pooled_prompt_embeds is not None:\n",
    "                pooled_prompt_embeds = pooled_prompt_embeds.to(device=device, dtype=pipeline_torch_dtype)\n",
    "\n",
    "            # Disable autocast for the pipeline call to avoid mixed-precision VAE ops\n",
    "            if device.startswith(\"cuda\"):\n",
    "                # from torch.amp import autocast\n",
    "                with autocast(\"cuda\", dtype=autocast_dtype, enabled=autocast_enabled):\n",
    "                    out = pipe(\n",
    "                        prompt_embeds=prompt_embeds,\n",
    "                        pooled_prompt_embeds=pooled_prompt_embeds,\n",
    "                        image=image,\n",
    "                        strength=strength,\n",
    "                        num_inference_steps=num_inference_steps,\n",
    "                        guidance_scale=guidance_scale,\n",
    "                        generator=generator,\n",
    "                        output_type=output_type\n",
    "                    )\n",
    "            else:\n",
    "                out = pipe(\n",
    "                    prompt_embeds=prompt_embeds,\n",
    "                    pooled_prompt_embeds=pooled_prompt_embeds,\n",
    "                    image=image,\n",
    "                    strength=strength,\n",
    "                    num_inference_steps=num_inference_steps,\n",
    "                    guidance_scale=guidance_scale,\n",
    "                    generator=generator,\n",
    "                    output_type=output_type\n",
    "                )\n",
    "\n",
    "            # Restore VAE dtype if we changed it\n",
    "            if need_cast_back:\n",
    "                vae.to(orig_vae_dtype)\n",
    "\n",
    "            return out\n",
    "\n",
    "        except Exception as e:\n",
    "            last_exc = e\n",
    "            # If the failure looks like the zero-reshape issue and we haven't yet retried\n",
    "            # with more steps, do that first (this often fixes small-step failures).\n",
    "            err_msg = str(e).lower()\n",
    "            is_reshape_zero_err = \"cannot reshape tensor of 0 elements\" in err_msg or \"shape [0\" in err_msg\n",
    "\n",
    "            if is_reshape_zero_err and (not attempted_with_increased_steps) and int(num_inference_steps) < int(retry_with_steps):\n",
    "                # Try again with a larger number of steps (keeps other normalization)\n",
    "                attempted_with_increased_steps = True\n",
    "                print(f\"[PIPE RETRY-STEPS] detected small-step reshape failure; retrying with {retry_with_steps} steps.\")\n",
    "                try:\n",
    "                    # Up the scheduler steps and try once more\n",
    "                    pipe.scheduler.set_timesteps(int(retry_with_steps))\n",
    "                except Exception:\n",
    "                    pass\n",
    "                # loop will retry automatically\n",
    "                continue\n",
    "\n",
    "            # Otherwise perform the previous normalization retry behavior\n",
    "            print(f\"[PIPE RETRY] attempt {attempt+1} failed: {e}. Retrying after re-normalize.\")\n",
    "            try:\n",
    "                arr = np.asarray(image).astype(np.uint8)\n",
    "                image = Image.fromarray(arr).convert(\"RGB\").resize((target_px, target_px), Image.LANCZOS)\n",
    "            except Exception as e2:\n",
    "                print(f\"[PIPE RETRY] aggressive normalize failed: {e2}\")\n",
    "\n",
    "            # restore VAE dtype before next attempt to keep pipeline consistent\n",
    "            try:\n",
    "                vae.to(orig_vae_dtype)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    # Final fallback: call pipeline with prompt text (let pipeline compute embeddings)\n",
    "    if prompt_text is not None:\n",
    "        try:\n",
    "            print(\"[PIPE FALLBACK] calling pipeline with prompt text fallback.\")\n",
    "            need_cast_back = False\n",
    "            if orig_vae_dtype == torch.float16:\n",
    "                vae.to(torch.float32)\n",
    "                need_cast_back = True\n",
    "\n",
    "            if device.startswith(\"cuda\"):\n",
    "                # from torch.amp import autocast\n",
    "                with autocast(\"cuda\", dtype=autocast_dtype, enabled=autocast_enabled):\n",
    "                    out = pipe(\n",
    "                        prompt=prompt_text,\n",
    "                        image=image,\n",
    "                        strength=strength,\n",
    "                        num_inference_steps=max(num_inference_steps, retry_with_steps),\n",
    "                        guidance_scale=guidance_scale,\n",
    "                        generator=generator,\n",
    "                        output_type=output_type\n",
    "                    )\n",
    "            else:\n",
    "                out = pipe(\n",
    "                    prompt=prompt_text,\n",
    "                    image=image,\n",
    "                    strength=strength,\n",
    "                    num_inference_steps=max(num_inference_steps, retry_with_steps),\n",
    "                    guidance_scale=guidance_scale,\n",
    "                    generator=generator,\n",
    "                    output_type=output_type\n",
    "                )\n",
    "\n",
    "            if need_cast_back:\n",
    "                vae.to(orig_vae_dtype)\n",
    "            return out\n",
    "        except Exception as e:\n",
    "            last_exc = e\n",
    "        finally:\n",
    "            try:\n",
    "                vae.to(orig_vae_dtype)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    raise RuntimeError(f\"image2image pipeline failed after retries: {last_exc}\")\n",
    "\n",
    "# -------------------------\n",
    "# Prepare prompts, seeds, images\n",
    "# -------------------------\n",
    "\n",
    "# Start total timer\n",
    "total_start = time.time()\n",
    "\n",
    "if not isinstance(prompts, (list, tuple)) or len(prompts) < 2:\n",
    "    raise ValueError(\"prompts must be a list with at least two items\")\n",
    "\n",
    "num_prompts = len(prompts)\n",
    "seeds_list = ensure_list_of_seeds(seeds, num_prompts)\n",
    "prompt_images_list = ensure_list_of_images(prompt_images, num_prompts)\n",
    "\n",
    "print(\"Encoding prompts...\")\n",
    "encoded_list, max_seq_len = encode_prompts(prompts)\n",
    "\n",
    "padded_prompts = []\n",
    "pooled_prompts = []\n",
    "for pe, pooled in encoded_list:\n",
    "    pe_padded = pad_prompt_embeds(pe, max_seq_len)\n",
    "    padded_prompts.append(pe_padded)\n",
    "    pooled_prompts.append(pooled)\n",
    "\n",
    "image_latents = []\n",
    "image_latent_flags = []\n",
    "image_pils = []\n",
    "for idx, img_path in enumerate(prompt_images_list):\n",
    "    if img_path is None:\n",
    "        image_latents.append(None)\n",
    "        image_latent_flags.append(False)\n",
    "        image_pils.append(None)\n",
    "    else:\n",
    "        print(f\"Encoding image to latent for prompt index {idx}: {img_path}\")\n",
    "        lat = encode_image_to_latent(img_path)\n",
    "        image_latents.append(lat)\n",
    "        image_latent_flags.append(True)\n",
    "        pil = latent_to_pil(lat)\n",
    "        image_pils.append(pil)\n",
    "\n",
    "# -------------------------\n",
    "# Generation loop\n",
    "# -------------------------\n",
    "print(\"Generating frames with image2image...\")\n",
    "frame_index = 0\n",
    "total_segments = num_prompts - 1\n",
    "\n",
    "shape = (\n",
    "    1,\n",
    "    pipe.unet.config.in_channels,\n",
    "    pipe.unet.config.sample_size,\n",
    "    pipe.unet.config.sample_size,\n",
    ")\n",
    "print(\"UNet latent shape expected:\", shape)\n",
    "\n",
    "with open(DEBUG_CSV, \"w\", newline=\"\") as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"frame_index\", \"segment\", \"step\", \"raw_t\", \"eased_t\", \"is_img_a\", \"is_img_b\",\n",
    "                     \"latent_min\", \"latent_max\", \"latent_mean\", \"latent_std\", \"time_s\"])\n",
    "\n",
    "prev_latents: Optional[torch.Tensor] = None\n",
    "\n",
    "pipe.scheduler.set_timesteps(num_inference_steps)\n",
    "timesteps = pipe.scheduler.timesteps\n",
    "t_start = timesteps[0]\n",
    "\n",
    "for seg_idx in range(total_segments):\n",
    "\n",
    "    seg_start = time.time()\n",
    "    \n",
    "    pe_a = padded_prompts[seg_idx]\n",
    "    pe_b = padded_prompts[seg_idx + 1]\n",
    "    pooled_a = pooled_prompts[seg_idx]\n",
    "    pooled_b = pooled_prompts[seg_idx + 1]\n",
    "\n",
    "    img_lat_a = image_latents[seg_idx]\n",
    "    img_lat_b = image_latents[seg_idx + 1]\n",
    "    pil_a = image_pils[seg_idx]\n",
    "    pil_b = image_pils[seg_idx + 1]\n",
    "    is_img_a = image_latent_flags[seg_idx]\n",
    "    is_img_b = image_latent_flags[seg_idx + 1]\n",
    "\n",
    "    steps_in_segment = num_steps_between\n",
    "\n",
    "    seed_a = seeds_list[seg_idx]\n",
    "    seed_b = seeds_list[seg_idx + 1]\n",
    "\n",
    "    combined_seed = (int(seed_a) * 31 + int(seed_b) * 17) & 0x7FFFFFFF\n",
    "\n",
    "    pipe.scheduler.set_timesteps(num_inference_steps)\n",
    "    timesteps = pipe.scheduler.timesteps\n",
    "    t_start = timesteps[0]\n",
    "\n",
    "    # If mixing image and noise, create scaled noise endpoints\n",
    "    if is_img_a and not is_img_b and img_lat_b is None:\n",
    "        tmp_gen_b = torch.Generator(device=device).manual_seed(int(seed_b))\n",
    "        img_lat_b = torch.randn(shape, generator=tmp_gen_b, device=device, dtype=torch.float32) * float(scheduler_init_sigma) * noise_mix_scale\n",
    "    if is_img_b and not is_img_a and img_lat_a is None:\n",
    "        tmp_gen_a = torch.Generator(device=device).manual_seed(int(seed_a))\n",
    "        img_lat_a = torch.randn(shape, generator=tmp_gen_a, device=device, dtype=torch.float32) * float(scheduler_init_sigma) * noise_mix_scale\n",
    "\n",
    "    for step in range(steps_in_segment + 1):\n",
    "\n",
    "        step_start = time.time()\n",
    "        \n",
    "        if step == steps_in_segment and seg_idx < total_segments - 1:\n",
    "            continue\n",
    "\n",
    "        raw_t = step / float(steps_in_segment)\n",
    "        eased_t = cosine_ease(raw_t)\n",
    "\n",
    "        cond_interp = lerp(pe_a, pe_b, eased_t)\n",
    "        pooled_interp = lerp(pooled_a, pooled_b, eased_t)\n",
    "\n",
    "        # Branch: both-noise segment\n",
    "        if (not is_img_a) and (not is_img_b):\n",
    "            if img_lat_a is None:\n",
    "                tmp_gen_a = torch.Generator(device=device).manual_seed(int(seed_a))\n",
    "                endpoint_a = torch.randn(shape, generator=tmp_gen_a, device=device, dtype=torch.float32) * float(scheduler_init_sigma)\n",
    "            else:\n",
    "                endpoint_a = img_lat_a.to(device=device, dtype=torch.float32)\n",
    "            if img_lat_b is None:\n",
    "                tmp_gen_b = torch.Generator(device=device).manual_seed(int(seed_b))\n",
    "                endpoint_b = torch.randn(shape, generator=tmp_gen_b, device=device, dtype=torch.float32) * float(scheduler_init_sigma)\n",
    "            else:\n",
    "                endpoint_b = img_lat_b.to(device=device, dtype=torch.float32)\n",
    "\n",
    "            latents = slerp(endpoint_a, endpoint_b, eased_t)\n",
    "\n",
    "            if prev_latents is not None and temporal_smooth_alpha > 0.0:\n",
    "                latents = prev_latents * temporal_smooth_alpha + latents * (1.0 - temporal_smooth_alpha)\n",
    "\n",
    "            if torch.isnan(latents).any():\n",
    "                latents = torch.nan_to_num(latents, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "            vae_scaling = getattr(pipe.vae.config, \"scaling_factor\", 0.13025)\n",
    "            lat_for_noise = latents / float(vae_scaling)\n",
    "\n",
    "            gen = torch.Generator(device=device).manual_seed(int((combined_seed + step) & 0x7FFFFFFF))\n",
    "            noise = torch.randn(lat_for_noise.shape, generator=gen, device=device, dtype=torch.float32)\n",
    "\n",
    "            try:\n",
    "                noisy_latents = pipe.scheduler.add_noise(lat_for_noise, noise, t_start)\n",
    "            except Exception:\n",
    "                noisy_latents = lat_for_noise + noise * float(scheduler_init_sigma)\n",
    "\n",
    "            pil_noisy = latent_to_pil(noisy_latents)\n",
    "\n",
    "            strength_use = max(min_strength, base_strength + 0.05)\n",
    "            frame_gen = torch.Generator(device=device).manual_seed(int((combined_seed + step) & 0x7FFFFFFF))\n",
    "            fallback_prompt_text = prompts[seg_idx] if raw_t < 0.5 else prompts[seg_idx + 1]\n",
    "\n",
    "            out = safe_call_pipe_with_image(\n",
    "                pipe,\n",
    "                prompt_embeds=cond_interp,\n",
    "                pooled_prompt_embeds=pooled_interp,\n",
    "                prompt_text=fallback_prompt_text,\n",
    "                image=pil_noisy,\n",
    "                strength=strength_use,\n",
    "                num_inference_steps=num_inference_steps,\n",
    "                guidance_scale=guidance_scale,\n",
    "                generator=frame_gen,\n",
    "                output_type=\"pil\"\n",
    "            )\n",
    "\n",
    "            image = out.images[0]\n",
    "            frame_path = os.path.join(output_dir, f\"frame_{frame_index:06d}.png\")\n",
    "            image.save(frame_path)\n",
    "            print(f\"Saved {frame_path}\")\n",
    "\n",
    "            try:\n",
    "                enc_lat = encode_image_to_latent(image)\n",
    "                prev_latents = enc_lat.to(device=device, dtype=torch.float32)\n",
    "            except Exception:\n",
    "                prev_latents = None\n",
    "\n",
    "            t0 = time.time()\n",
    "            if prev_latents is not None:\n",
    "                lmin = float(prev_latents.min().cpu())\n",
    "                lmax = float(prev_latents.max().cpu())\n",
    "                lmean = float(prev_latents.mean().cpu())\n",
    "                lstd = float(prev_latents.std().cpu())\n",
    "            else:\n",
    "                lmin = lmax = lmean = lstd = 0.0\n",
    "            with open(DEBUG_CSV, \"a\", newline=\"\") as csvfile:\n",
    "                writer = csv.writer(csvfile)\n",
    "                writer.writerow([frame_index, seg_idx, step, f\"{raw_t:.6f}\", f\"{eased_t:.6f}\", int(is_img_a), int(is_img_b),\n",
    "                                 f\"{lmin:.8f}\", f\"{lmax:.8f}\", f\"{lmean:.8f}\", f\"{lstd:.8f}\", f\"{t0:.6f}\"])\n",
    "\n",
    "            if frame_index < DEBUG_FRAMES:\n",
    "                print(f\"DEBUG FRAME {frame_index} seg {seg_idx} step {step}/{steps_in_segment} raw_t={raw_t:.4f} eased_t={eased_t:.4f}\")\n",
    "                print(\"  pil_noisy size\", pil_noisy.size)\n",
    "                print(f\"  latent stats min {lmin:.6f} max {lmax:.6f} mean {lmean:.6f} std {lstd:.6f}\")\n",
    "\n",
    "            frame_index += 1\n",
    "            step_end = time.time()\n",
    "            step_runtime = step_end - step_start\n",
    "            print('-' * 70)\n",
    "            print(f\"Step {step} runtime: {step_runtime:.6f} sec\")\n",
    "            print('-' * 70)\n",
    "            continue\n",
    "\n",
    "        # Branch: at least one side is an image -> pixel-space interpolation\n",
    "        if pil_a is None and pil_b is None:\n",
    "            pil_a = pil_b = Image.new(\"RGB\", (pipe.unet.config.sample_size * 8, pipe.unet.config.sample_size * 8), (127, 127, 127))\n",
    "        if pil_a is None:\n",
    "            pil_a = pil_b.copy()\n",
    "        if pil_b is None:\n",
    "            pil_b = pil_a.copy()\n",
    "\n",
    "        target_px = pipe.unet.config.sample_size * 8\n",
    "        if pil_a.size != (target_px, target_px):\n",
    "            pil_a = pil_a.resize((target_px, target_px), resample=Image.LANCZOS)\n",
    "        if pil_b.size != (target_px, target_px):\n",
    "            pil_b = pil_b.resize((target_px, target_px), resample=Image.LANCZOS)\n",
    "\n",
    "        a_arr = np.asarray(pil_a).astype(np.float32) / 255.0\n",
    "        b_arr = np.asarray(pil_b).astype(np.float32) / 255.0\n",
    "        interp_arr = (1.0 - eased_t) * a_arr + eased_t * b_arr\n",
    "        interp_arr = (interp_arr * 255.0).clip(0, 255).astype(\"uint8\")\n",
    "        pil_noisy = Image.fromarray(interp_arr)\n",
    "\n",
    "        strength = base_strength * (1.0 - 0.5 * (1.0 - math.cos(math.pi * raw_t)))\n",
    "        strength = max(min_strength, float(strength))\n",
    "\n",
    "        frame_gen = torch.Generator(device=device).manual_seed(int((combined_seed + step) & 0x7FFFFFFF))\n",
    "        fallback_prompt_text = prompts[seg_idx] if raw_t < 0.5 else prompts[seg_idx + 1]\n",
    "\n",
    "        out = safe_call_pipe_with_image(\n",
    "            pipe,\n",
    "            prompt_embeds=cond_interp,\n",
    "            pooled_prompt_embeds=pooled_interp,\n",
    "            prompt_text=fallback_prompt_text,\n",
    "            image=pil_noisy,\n",
    "            strength=strength,\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            guidance_scale=guidance_scale,\n",
    "            generator=frame_gen,\n",
    "            output_type=\"pil\"\n",
    "        )\n",
    "\n",
    "        image = out.images[0]\n",
    "        frame_path = os.path.join(output_dir, f\"frame_{frame_index:06d}.png\")\n",
    "        image.save(frame_path)\n",
    "        print(f\"Saved {frame_path}\")\n",
    "\n",
    "        try:\n",
    "            enc_lat = encode_image_to_latent(image)\n",
    "            prev_latents = enc_lat.to(device=device, dtype=torch.float32)\n",
    "        except Exception:\n",
    "            prev_latents = None\n",
    "\n",
    "        t0 = time.time()\n",
    "        if prev_latents is not None:\n",
    "            lmin = float(prev_latents.min().cpu())\n",
    "            lmax = float(prev_latents.max().cpu())\n",
    "            lmean = float(prev_latents.mean().cpu())\n",
    "            lstd = float(prev_latents.std().cpu())\n",
    "        else:\n",
    "            lmin = lmax = lmean = lstd = 0.0\n",
    "        with open(DEBUG_CSV, \"a\", newline=\"\") as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow([frame_index, seg_idx, step, f\"{raw_t:.6f}\", f\"{eased_t:.6f}\", int(is_img_a), int(is_img_b),\n",
    "                             f\"{lmin:.8f}\", f\"{lmax:.8f}\", f\"{lmean:.8f}\", f\"{lstd:.8f}\", f\"{t0:.6f}\"])\n",
    "\n",
    "        if frame_index < DEBUG_FRAMES:\n",
    "            print(f\"DEBUG FRAME {frame_index} seg {seg_idx} step {step}/{steps_in_segment} raw_t={raw_t:.4f} eased_t={eased_t:.4f}\")\n",
    "            print(\"  pil_noisy size\", pil_noisy.size)\n",
    "            print(f\"  latent stats min {lmin:.6f} max {lmax:.6f} mean {lmean:.6f} std {lstd:.6f}\")\n",
    "\n",
    "        frame_index += 1\n",
    "\n",
    "        step_end = time.time()\n",
    "        step_runtime = step_end - step_start\n",
    "        print('-' * 70)\n",
    "        print(f\"Step {step} runtime: {step_runtime:.6f} sec\")\n",
    "        print('-' * 70)\n",
    "\n",
    "    seg_end = time.time()\n",
    "    seg_runtime = seg_end - seg_start\n",
    "    print('=' * 70)\n",
    "    print(f\"Segment {seg_idx} runtime: {seg_runtime:.6f} sec\")\n",
    "    print('=' * 70)\n",
    "\n",
    "# -------------------------\n",
    "# Assemble video\n",
    "# -------------------------\n",
    "print(\"Assembling video...\")\n",
    "frames = sorted([os.path.join(output_dir, p) for p in os.listdir(output_dir) if p.startswith(\"frame_\") and p.endswith(\".png\")])\n",
    "if not frames:\n",
    "    raise RuntimeError(\"No frames found\")\n",
    "\n",
    "try:\n",
    "    from moviepy.video.io.ImageSequenceClip import ImageSequenceClip\n",
    "    clip = ImageSequenceClip(frames, fps=framerate)\n",
    "    clip.write_videofile(output_video, codec=\"libx264\", audio=False, logger=None)\n",
    "    print(f\"Saved video to {output_video} using moviepy\")\n",
    "except Exception as e:\n",
    "    print(\"moviepy failed, trying ffmpeg:\", str(e))\n",
    "    input_pattern = os.path.join(output_dir, \"frame_%06d.png\")\n",
    "    import subprocess\n",
    "    subprocess.run(\n",
    "        [\n",
    "            \"ffmpeg\", \"-y\", \"-framerate\", str(framerate),\n",
    "            \"-i\", input_pattern, \"-c:v\", \"libx264\",\n",
    "            \"-pix_fmt\", \"yuv420p\", output_video\n",
    "        ],\n",
    "        check=True\n",
    "    )\n",
    "    print(f\"Saved video to {output_video} using ffmpeg\")\n",
    "\n",
    "if cleanup_frames_after_video:\n",
    "    for p in frames:\n",
    "        try:\n",
    "            os.remove(p)\n",
    "        except Exception:\n",
    "            pass\n",
    "    print(\"Cleaned up intermediate PNG frames\")\n",
    "\n",
    "\n",
    "# End total timer\n",
    "total_end = time.time()\n",
    "total_runtime = total_end - total_start\n",
    "\n",
    "print('=' * 70)\n",
    "print(f\"\\nTotal runtime: {total_runtime:.6f} sec\")\n",
    "print('=' * 70)\n",
    "print(\"Done.\")\n",
    "print('=' * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af6f0e7-b728-4edf-b986-5e8516284f88",
   "metadata": {},
   "source": [
    "# Congrats! You did it :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
